{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-09T11:33:07.469893500Z",
     "start_time": "2025-03-09T11:33:06.788612700Z"
    }
   },
   "id": "cb34f9db4e74995f"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  10\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "with open('./texts.txt', mode='r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "punctuation_chars = r'[!?.]'\n",
    "sentences = re.split(punctuation_chars, text)\n",
    "\n",
    "print('Number of sentences: ', len([x for x in sentences if x.strip()]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-09T11:12:32.761234200Z",
     "start_time": "2025-03-09T11:12:32.748851900Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:  160\n",
      "Number of unique words:  97\n"
     ]
    }
   ],
   "source": [
    "new_text = text.replace('\\n', ' ')\n",
    "new_text = new_text.replace('.', ' ')\n",
    "new_text = new_text.replace(':', ' ')\n",
    "new_text = new_text.replace('?', ' ')\n",
    "new_text = new_text.replace('\\\"', ' ')\n",
    "new_text = new_text.replace('!', ' ')\n",
    "new_text = new_text.replace(',', ' ')\n",
    "new_text = new_text.replace('-', ' ')\n",
    "new_text = new_text.replace('”', ' ')\n",
    "\n",
    "unique_words = []\n",
    "for word in [x for x in new_text.split(' ') if x.strip()]:\n",
    "    if word not in unique_words:\n",
    "        unique_words += [word]\n",
    "        \n",
    "print('Number of words: ', len([x for x in new_text.split(' ') if x.strip()]))   \n",
    "print('Number of unique words: ', len(unique_words))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-09T11:14:34.243563Z",
     "start_time": "2025-03-09T11:14:34.165258500Z"
    }
   },
   "id": "17ec1e42b39fd5e6"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest word/words:  o, e\n",
      "Longest word/words:  Confiiiiiiiiiiiiiiiiiiiiiiiiiiiirm\n"
     ]
    }
   ],
   "source": [
    "shortest_words = [unique_words[0]]\n",
    "longest_words = [unique_words[0]]\n",
    "\n",
    "for word in [w for w in unique_words if all(not c.isdigit() for c in w)]:\n",
    "    if len(word) < len(shortest_words[0]):\n",
    "        shortest_words = [word]\n",
    "    if len(word) > len(longest_words[0]):\n",
    "        longest_words = [word]\n",
    "    if len(word) == len(shortest_words[0]) and word != shortest_words[0]:\n",
    "        shortest_words += [word]\n",
    "    if len(word) == len(longest_words[0]) and word != longest_words[0]:\n",
    "        longest_words += [word]\n",
    "\n",
    "print('Shortest word/words: ', \", \".join(shortest_words))\n",
    "print('Longest word/words: ', \", \".join(longest_words))\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-09T11:21:19.646150800Z",
     "start_time": "2025-03-09T11:21:19.562700600Z"
    }
   },
   "id": "9525887c60563b20"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mesaj de informare: \n",
      "Cursul si laboratoarele de Inteligenta Artificiala vor fi o \n",
      "provocare pentru toti. Suntem convinsi ca veti realiza proiecte \n",
      "foarte interesante. Va incurajam sa adresati intrebari atunci \n",
      "cand ceva nu e clar, atat in mod live, cat si folosind platforma \n",
      "Teams, canalul ”general”. \n",
      "Daca ati citit pana aici, va rugam sa lasati un mesaj pe canalul \n",
      "general cu textul ”Confiiiiiiiiiiiiiiiiiiiiiiiiiiiirm ca am citit \n",
      "textul pentru problema 3 din lab2”. \n",
      "--\n",
      "Mesaj de informare generat de ChatGPT:\n",
      "Stimati cursanti,\n",
      "Suntem incantati sa va avem in echipa noastra pentru Cursul si \n",
      "laboratoarele de Inteligenta Artificiala. Aceasta experienta va \n",
      "fi o adevarata provocare, dar suntem convinsi ca veti realiza \n",
      "proiecte extrem de interesante.\n",
      "Va incurajam sa fiti activi si sa adresati intrebari atunci cand \n",
      "ceva nu este clar. Fie ca este vorba de o discutie in timp real \n",
      "sau prin intermediul platformei Teams, canalul ”general”, suntem \n",
      "aici sa va sprijinim.\n",
      "Succes si sa inceapa aventura AI!\n",
      "Cu consideratie, Echipa de Inteligenta Artificiala\n"
     ]
    }
   ],
   "source": [
    "text_without_special = text.replace('ș', 's')\n",
    "text_without_special = text_without_special.replace('ț', 't')\n",
    "text_without_special = text_without_special.replace('ă', 'a')\n",
    "text_without_special = text_without_special.replace('â', 'a')\n",
    "text_without_special = text_without_special.replace('î', 'i')\n",
    "text_without_special = text_without_special.replace('Ș', 'S')\n",
    "text_without_special = text_without_special.replace('Ț', 'T')\n",
    "text_without_special = text_without_special.replace('Ă', 'A')\n",
    "text_without_special = text_without_special.replace('Â', 'A')\n",
    "text_without_special = text_without_special.replace('Î', 'I')\n",
    "\n",
    "print(text_without_special)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-09T11:27:40.686683500Z",
     "start_time": "2025-03-09T11:27:40.596523700Z"
    }
   },
   "id": "d1a3937a40372662"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym for Confiiiiiiiiiiiiiiiiiiiiiiiiiiiirm is/are: \n",
      "Synonym for Confirm is/are: substantiate, support, confirm, corroborate, sustain, reassert, affirm\n"
     ]
    }
   ],
   "source": [
    "for word in longest_words + ['Confirm']:\n",
    "    synonyms = set()\n",
    "    for s in wordnet.synsets(word):\n",
    "        for l in s.lemmas():\n",
    "            synonyms.add(l.name())\n",
    "    print(f'Synonym for {word} is/are: {\", \".join(synonyms)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-09T11:40:10.385324600Z",
     "start_time": "2025-03-09T11:40:10.374370900Z"
    }
   },
   "id": "8b0f06080fa5c261"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
