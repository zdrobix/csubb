{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-15T13:07:47.523671800Z",
     "start_time": "2025-05-15T13:07:47.492120400Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utils"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ec008aeeb169cdc"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "def clean_data(review):\n",
    "    if isinstance(review, str):\n",
    "        no_punc = re.sub(r'[^\\w\\s]', '', review)\n",
    "        return ''.join([i for i in no_punc if not i.isdigit()])\n",
    "    return ''\n",
    "\n",
    "def read_dataset():\n",
    "    df = pd.read_csv(\"./data/emotions_labeled.csv\")\n",
    "    return df\n",
    "\n",
    "def train_test_split(set1, set2, test_size, random_state=21):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.arange(len(set1))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    test_size = int(test_size * len(set1))\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    \n",
    "    if isinstance(set1, pd.DataFrame):\n",
    "        x_train = set1.iloc[train_indices]\n",
    "        x_test = set1.iloc[test_indices]\n",
    "    else:\n",
    "        x_train = set1[train_indices]\n",
    "        x_test = set1[test_indices]\n",
    "\n",
    "    y_train = set2[train_indices]\n",
    "    y_test = set2[test_indices]\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "emotions_label = {\n",
    "    0: 'sadness',\n",
    "    1: 'joy',\n",
    "    2: 'love', \n",
    "    3: 'anger', \n",
    "    4: 'fear',\n",
    "    5: 'surprise'\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-15T14:10:10.024959500Z",
     "start_time": "2025-05-15T14:10:09.960689300Z"
    }
   },
   "id": "41a6e40a4bc5810e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Vectorizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95cba3716bcfb99"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "class TfVectorizer:\n",
    "    def __init__(self, max_features= None):\n",
    "        self.max_features = max_features\n",
    "        self.words = {}\n",
    "        self.idFreq = {}\n",
    "        \n",
    "    def _tokenize(self, text):\n",
    "        return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    \n",
    "    def _extract_punctuation_features(self, text):\n",
    "        exclam_count = text.count('!') / len(text)\n",
    "        question_count = text.count('?') / len(text)\n",
    "        return [exclam_count, question_count]\n",
    "    \n",
    "    def fit(self, raw_texts):\n",
    "        dict = defaultdict(int)\n",
    "        texts_count = len(raw_texts)\n",
    "        token_count = []\n",
    "        for text in raw_texts:\n",
    "            tokenized = set(self._tokenize(text))\n",
    "            token_count += [tokenized]\n",
    "            for token in tokenized:\n",
    "                dict[token] += 1\n",
    "        \n",
    "        sorted_terms = sorted(dict.items(), key= lambda x: -x[1])\n",
    "        if self.max_features:\n",
    "            sorted_terms = sorted_terms[:self.max_features]\n",
    "            \n",
    "        self.words = {term: idx for idx, (term, _) in enumerate(sorted_terms)}\n",
    "        self.idFreq = {\n",
    "            term: math.log(1 + texts_count) / (1 + dict[term]) + 1\n",
    "            for term in self.words\n",
    "        }\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, raw_texts):\n",
    "        rows = []\n",
    "        for text in raw_texts:\n",
    "            dict = defaultdict(int)\n",
    "            tokenized = self._tokenize(text)\n",
    "            for token in tokenized:\n",
    "                if token in self.words:\n",
    "                    dict[token] += 1\n",
    "                    \n",
    "            max_dict = max(dict.values()) if dict else 1\n",
    "            row = np.zeros(len(self.words))\n",
    "            \n",
    "            for token, count in dict.items():\n",
    "                if token in self.words:\n",
    "                    dict_val = count / max_dict\n",
    "                    idFreq_val = self.idFreq[token]\n",
    "                    idx = self.words[token]\n",
    "                    row[idx] = dict_val * idFreq_val\n",
    "                    \n",
    "            punctuation_features = self._extract_punctuation_features(text)\n",
    "            row = np.concatenate([row, punctuation_features])\n",
    "                    \n",
    "            rows.append(row)\n",
    "        return np.array(rows)\n",
    "    \n",
    "    def fit_transform(self, raw_texts):\n",
    "        self.fit(raw_texts)\n",
    "        return self.transform(raw_texts)\n",
    "                \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-15T13:58:33.026851500Z",
     "start_time": "2025-05-15T13:58:32.949956400Z"
    }
   },
   "id": "ca938c0c1b9c01dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train test split"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ef5e78b5a3a2b2e"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "vectorizer = TfVectorizer(max_features=1000)\n",
    "dataset = read_dataset()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset['text'], dataset['label'],  test_size=0.2, random_state=21)\n",
    "\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "x_test = vectorizer.transform(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-15T13:59:23.083492700Z",
     "start_time": "2025-05-15T13:58:50.777551300Z"
    }
   },
   "id": "870656bdf52eaa33"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ANN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee84854ed0f7f442"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 hidden_size,\n",
    "                 output_size,\n",
    "                 learning_rate= .01,\n",
    "                 epochs= 100,\n",
    "                 print= True):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size)  * np.sqrt(2. / hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        self.print = print\n",
    "        self.loss_list = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        for epoch in range(self.epochs):\n",
    "            z1 = np.dot(X, self.W1) + self.b1\n",
    "            a1 = relu(z1)\n",
    "            z2 = np.dot(a1, self.W2) + self.b2\n",
    "            a2 = self.softmax(z2)\n",
    "            \n",
    "            loss = -np.mean(np.sum(y * np.log(a2 + 1e-8), axis=1))\n",
    "            self.loss_list += [loss]\n",
    "            if self.print:\n",
    "                print(f\"Epoch {epoch}) -> Loss: {loss:.6f}\")\n",
    "            \n",
    "            dz2 = a2 - y\n",
    "            dW2 = np.dot(a1.T, dz2)\n",
    "            db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "            dz1 = np.dot(dz2, self.W2.T) * relu_deriv(a1)\n",
    "            dW1 = np.dot(X.T, dz1)\n",
    "            db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "            self.W2 -= self.learning_rate * dW2\n",
    "            self.b2 -= self.learning_rate * db2\n",
    "            self.W1 -= self.learning_rate * dW1\n",
    "            self.b1 -= self.learning_rate * db1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        z1 = np.dot(X, self.W1) + self.b1\n",
    "        a1 = relu(z1)\n",
    "        z2 = np.dot(a1, self.W2) + self.b2\n",
    "        a2 = self.softmax(z2)\n",
    "        return np.argmax(a2, axis= 1)\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  \n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def to_one_hot(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels.reshape(-1)]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-15T13:59:31.712769500Z",
     "start_time": "2025-05-15T13:59:31.694881100Z"
    }
   },
   "id": "71eb5f08e98d9fa4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33b8bb1de8986c1f"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0) -> Loss: 1.786638\n",
      "Epoch 1) -> Loss: 1.641715\n",
      "Epoch 2) -> Loss: 1.681869\n",
      "Epoch 3) -> Loss: 1.584947\n",
      "Epoch 4) -> Loss: 1.569892\n",
      "Epoch 5) -> Loss: 1.568530\n",
      "Epoch 6) -> Loss: 1.566445\n",
      "Epoch 7) -> Loss: 1.569076\n",
      "Epoch 8) -> Loss: 1.568035\n",
      "Epoch 9) -> Loss: 1.574361\n"
     ]
    }
   ],
   "source": [
    "# y_train = y_train.to_numpy().reshape(-1, 1)\n",
    "# \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "\n",
    "classifier = ANN(input_size=1002, hidden_size=128, epochs= 10, output_size= 6, learning_rate= 1e-5)\n",
    "classifier.fit(x_train, y_train_onehot)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-15T14:08:47.155521500Z",
     "start_time": "2025-05-15T14:08:11.970491500Z"
    }
   },
   "id": "489a111c4db63b03"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test trained model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae3243dacbdb7518"
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9929\n"
     ]
    }
   ],
   "source": [
    "y_predict = classifier.predict(x_test)\n",
    "# y_test = y_test.to_numpy().reshape(-1, 1)\n",
    "y_test_idx = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = np.mean(y_predict == y_test_idx)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-15T14:08:56.276131500Z",
     "start_time": "2025-05-15T14:08:55.110179500Z"
    }
   },
   "id": "e49d574d5c36b527"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Given the text, the model predicted "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c646d24cf3f4e608"
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotion:  joy\n"
     ]
    }
   ],
   "source": [
    "phrase = 'By choosing a bike over a car, I’m reducing my environmental footprint. Cycling promotes eco-friendly transportation, and I’m proud to be part of that movement..'\n",
    "\n",
    "new_data = pd.DataFrame({'text': [phrase]})\n",
    "new_data['text'] = new_data['text'].apply(clean_data)\n",
    "\n",
    "x_new = vectorizer.transform(new_data['text'])\n",
    "\n",
    "predictions = classifier.predict(x_new)\n",
    "\n",
    "print(\"Predicted emotion: \", emotions_label[predictions[0]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-15T14:10:21.225000900Z",
     "start_time": "2025-05-15T14:10:21.207345800Z"
    }
   },
   "id": "1096f3e4f3edac3b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
