# SoluÈ›ie CUDA - ConvoluÈ›ie 2D OptimizatÄƒ

## ğŸ“‹ CerinÈ›a Laboratorului

**PostcondiÈ›ie**: Matricea iniÈ›ialÄƒ conÈ›ine imaginea filtratÄƒ.

**ConstrÃ¢ngere**: NU se alocÄƒ o altÄƒ matrice rezultat È™i nici o matrice temporarÄƒ!  
Se pot folosi/aloca doar vectori temporari pentru care complexitatea spaÈ›iu se Ã®ncadreazÄƒ Ã®n **O(n)**.

## âœ… SoluÈ›ia ImplementatÄƒ

### 1ï¸âƒ£ **Arhitectura SoluÈ›iei**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Host (CPU)â”‚
â”‚  f[n][m]    â”‚  â”€â”€â”
â”‚  c[k][k]    â”‚    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ Transfer
                   â”‚ CPUâ†’GPU
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Device (GPU)             â”‚
â”‚                                 â”‚
â”‚  d_f[n*m]      â† Date originale â”‚
â”‚  d_result[n*m] â† Buffer auxiliarâ”‚
â”‚  d_c[k*k]      â† Kernel         â”‚
â”‚                                 â”‚
â”‚  ğŸ”¹ Kernel paralel 2D           â”‚
â”‚     (nÃ—m threads)               â”‚
â”‚                                 â”‚
â”‚  d_result â†’ d_f  (copy)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”‚ Transfer
                   â”‚ GPUâ†’CPU
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Host (CPU)â”‚
â”‚  f[n][m]    â”‚ â† Rezultat final
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2ï¸âƒ£ **Algoritm CUDA**

#### **Kernel Paralel** (`convolution_kernel`)

```cuda
__global__ void convolution_kernel(int *d_f, int *d_result, int *d_c, int n, int m, int k)
{
    // Fiecare thread proceseazÄƒ o celulÄƒ (i, j)
    int i = blockIdx.y * blockDim.y + threadIdx.y;  // Linia
    int j = blockIdx.x * blockDim.x + threadIdx.x;  // Coloana

    if (i >= n || j >= m) return;

    // ConvoluÈ›ie 2D cu boundary conditions (clamp)
    int s = 0;
    int diff = (k - 1) / 2;

    for (int x = 0; x < k; x++) {
        for (int y = 0; y < k; y++) {
            int ii = clamp(i - diff + x, 0, n-1);
            int jj = clamp(j - diff + y, 0, m-1);
            s += d_f[ii * m + jj] * d_c[x * k + y];
        }
    }

    // Scrie Ã®n buffer auxiliar (evitÄƒ race conditions)
    d_result[i * m + j] = s;
}
```

#### **PaÈ™i de ExecuÈ›ie**

1. **Alocare memorie GPU**:

   - `d_f`: Matrice originalÄƒ (nÃ—m)
   - `d_result`: Buffer temporar pentru rezultat (nÃ—m)
   - `d_c`: Kernel de convoluÈ›ie (kÃ—k)

2. **Transfer date CPUâ†’GPU**:

   - Flatten f[n][m] â†’ f_flat[n*m]
   - Flatten c[k][k] â†’ c_flat[k*k]
   - cudaMemcpy â†’ GPU

3. **Lansare kernel paralel**:

   ```cuda
   dim3 threadsPerBlock(16, 16);  // 256 threads/block
   dim3 blocksPerGrid((m+15)/16, (n+15)/16);
   convolution_kernel<<<blocksPerGrid, threadsPerBlock>>>(d_f, d_result, d_c, n, m, k);
   ```

   - **ProceseazÄƒ toate cele nÃ—m celule Ã®n paralel**
   - Fiecare thread calculeazÄƒ convoluÈ›ia pentru o celulÄƒ

4. **Copiere rezultat**:

   - d_result â†’ d_f (GPU to GPU)
   - d_f â†’ f_flat (GPU to CPU)

5. **Cleanup**: Eliberare memorie GPU

### 3ï¸âƒ£ **Complexitate SpaÈ›iu**

#### **Pe CPU (Host)**:

- f[n][m]: O(nÃ—m) - **OBLIGATORIU** (matricea originalÄƒ)
- c[k][k]: O(kÂ²) - **OBLIGATORIU** (kernel-ul)
- f_flat[nÃ—m]: O(nÃ—m) - temporar pentru transfer
- c_flat[kÂ²]: O(kÂ²) - temporar pentru transfer
- **Total**: O(nÃ—m + kÂ²)

#### **Pe GPU (Device)**:

- d_f: O(nÃ—m) - matricea originalÄƒ
- d_result: O(nÃ—m) - **BUFFER AUXILIAR** (respectÄƒ constrÃ¢ngerea!)
- d_c: O(kÂ²) - kernel
- **Total**: O(nÃ—m + kÂ²)

âœ… **RespectÄƒ constrÃ¢ngerea**: Nu alocÄƒm matrice temporarÄƒ pe CPU, folosim doar buffer pe GPU!

### 4ï¸âƒ£ **Evitarea Race Conditions**

**ProblemÄƒ**: DacÄƒ scriem direct Ã®n `d_f` Ã®n timp ce citim din `d_f`, pot apÄƒrea race conditions cÃ¢nd thread-uri diferite acceseazÄƒ aceleaÈ™i zone.

**SoluÈ›ie**:

```cuda
// âŒ GreÈ™it: citire + scriere Ã®n aceeaÈ™i matrice
d_f[i * m + j] = calculate_convolution(d_f, ...);  // RACE CONDITION!

// âœ… Corect: citire din d_f, scriere Ã®n d_result
s = calculate_convolution(d_f, ...);
d_result[i * m + j] = s;  // FÄƒrÄƒ race conditions!

// Apoi copiem d_result â†’ d_f
cudaMemcpy(d_f, d_result, ..., cudaMemcpyDeviceToDevice);
```

### 5ï¸âƒ£ **DiferenÈ›e faÈ›Äƒ de implementarea C++**

| Aspect                | C++ (Multi-threading)            | CUDA (GPU)               |
| --------------------- | -------------------------------- | ------------------------ |
| **Paralelism**        | p thread-uri CPU                 | nÃ—m thread-uri GPU       |
| **Procesare**         | Linie cu linie + sincronizare    | Toate celulele simultan  |
| **Memorie auxiliarÄƒ** | 3 vectori Ã— m (prev, curr, next) | 1 matrice nÃ—m (d_result) |
| **Sincronizare**      | Barrier Ã®ntre thread-uri         | Implicit Ã®n kernel       |
| **Boundary handling** | LogicÄƒ cu vectori auxiliari      | Clamp direct Ã®n kernel   |

## ğŸ¯ **Avantaje SoluÈ›iei CUDA**

âœ… **Paralelism masiv**: ProceseazÄƒ toate nÃ—m celule simultan  
âœ… **FÄƒrÄƒ race conditions**: Buffer separat pentru scriere  
âœ… **Simplitate**: Un singur kernel launch  
âœ… **PerformanÈ›Äƒ**: ExploateazÄƒ complet GPU-ul  
âœ… **RespectÄƒ constrÃ¢ngerea**: Memorie O(nÃ—m) pe GPU

## ğŸ“Š **Testare**

### Cazuri de Test

```bash
# Test 1: N=M=10, k=3
nvcc -o main.exe main.cu
.\main.exe

# Test 2: N=M=1000, k=3
# Test 3: N=M=10000, k=3
```

### Verificare Corectitudine

```bash
# ComparÄƒ output-ul CUDA cu output-ul secvenÈ›ial C++
fc cuda\output.txt cpp\output_seq.txt
```

## ğŸ”§ **Compilare & Rulare**

```powershell
# Compilare
nvcc -o main.exe main.cu

# Rulare simplÄƒ
.\main.exe

# Rulare cu script (10 rulÄƒri + medie)
.\scriptCUDA.ps1 main.exe 0 10
```

## ğŸ“ **Verificare MatematicÄƒ**

Pentru o celulÄƒ (i, j), convoluÈ›ia este:

```
f'[i][j] = Î£ Î£ f[i-1+x][j-1+y] Ã— c[x][y]
           x y

unde x âˆˆ [0, k), y âˆˆ [0, k)
```

Boundary conditions: `clamp(index, 0, max-1)`

âœ… Implementarea CUDA respectÄƒ exact aceastÄƒ formulÄƒ!
